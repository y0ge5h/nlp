{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentiment Analysis, or Opinion Mining, is a sub-field of Natural Language Processing (NLP) that tries to identify and extract opinions within a given text. The aim of sentiment analysis is to gauge the attitude, sentiments, evaluations, attitudes and emotions of a speaker/writer based on the computational treatment of subjectivity in a text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why is sentiment analysis so important?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Businesses today are heavily dependent on data. Majority of this data however, is unstructured text coming from sources like emails, chats, social media, surveys, articles, and documents. The micro-blogging content coming from Twitter and Facebook poses serious challenges, not only because of the amount of data involved, but also because of the kind of language used in them to express sentiments, i.e., short forms, memes and emoticons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentiment Analysis is also useful for practitioners and researchers, especially in fields like sociology, marketing, advertising, psychology, economics, and political science, which rely a lot on human-computer interaction data.\n",
    "\n",
    "Sentiment Analysis enables companies to make sense out of data by being able to automate this entire process! Thus they are able to elicit vital insights from a vast unstructured dataset without having to manually indulge with it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why is Sentiment Analysis a Hard to perform Task?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Though it may seem easy on paper, Sentiment Analysis is actually a tricky subject. There are various reasons for that:\n",
    "\n",
    "Understanding emotions through text are not always easy. Sometimes even humans can get misled, so expecting a 100% accuracy from a computer is like asking for the Moon!\n",
    "A text may contain multiple sentiments all at once. For instance,\n",
    "â€œThe intent behind the movie was great, but it could have been betterâ€.\n",
    "\n",
    "The above sentence consists of two polarities, i.e., Positive as well as Negative. So how do we conclude whether the review was Positive or Negative?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computers arenâ€™t too comfortable in comprehending Figurative Speech. Figurative language uses words in a way that deviates from their conventionally accepted definitions in order to convey a more complicated meaning or heightened effect. Use of similes, metaphors, hyperboles etc qualify for a figurative speech. Let us understand it better with an example.\n",
    "â€œThe best I can say about the movie is that it was interesting.â€\n",
    "Here, the word â€™interestingâ€™ does not necessarily convey positive sentiment and can be confusing for algorithms.\n",
    "\n",
    "Heavy use of emoticons and slangs with sentiment values in social media texts like that of Twitter and Facebook also makes text analysis difficult. For example a â€œ :)â€ denotes a smiley and generally refers to positive sentiment while â€œ:(â€ denotes a negative sentiment on the other hand. Also, acronyms like â€œLOLâ€œ, â€OMGâ€ and commonly used slangs like â€œNahâ€, â€œmehâ€, â€gigglyâ€ etc are also strong indicators of some sort of sentiment in a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About TextBlob.\n",
    "TextBlob is a python library and offers a simple API to access its methods and perform basic NLP tasks. \n",
    "\n",
    "A good thing about TextBlob is that they are just like python strings. So, you can transform and play with it same like we did in python. Below, I have shown you below some basic tasks. Donâ€™t worry about the syntax, it is just to give you an intuition about how much-related TextBlob is to Python strings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment Setup\n",
    "Installation of TextBlob in your system in a simple task, all you need to do is open anaconda prompt ( or terminal if using Mac OS or Ubuntu) and enter the following commands:\n",
    "\n",
    "pip install -U textblob\n",
    "This will install TextBlob. For the uninitiated â€“ practical work in Natural Language Processing typically uses large bodies of linguistic data, or corpora. To download the necessary corpora, you can run the following command\n",
    "\n",
    "python -m textblob.download_corpora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features\n",
    "Noun phrase extraction\n",
    "Part-of-speech tagging\n",
    "Sentiment analysis\n",
    "Classification (Naive Bayes, Decision Tree)\n",
    "Language translation and detection powered by Google Translate\n",
    "Tokenization (splitting text into words and sentences)\n",
    "Word and phrase frequencies\n",
    "Parsing\n",
    "n-grams\n",
    "Word inflection (pluralization and singularization) and lemmatization\n",
    "Spelling correction\n",
    "Add new models or languages through extensions\n",
    "WordNet integration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP tasks using TextBlob\n",
    "Tokenization\n",
    "Tokenization refers to dividing text or a sentence into a sequence of tokens, which roughly correspond to â€œwordsâ€. This is one of the basic tasks of NLP. To do this using TextBlob, follow the two steps:\n",
    "\n",
    "Create a textblob object and pass a string with it.\n",
    "Call functions of textblob in order to do a specific task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "blob = TextBlob(\"GreyAtom is a great platform to learn data science. \\n It helps community through blogs, hackathons, discussions,etc.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Sentence(\"GreyAtom is a great platform to learn data science.\"),\n",
       " Sentence(\"It helps community through blogs, hackathons, discussions,etc.\")]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blob.sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Noun Phrase Extraction\n",
    "Since we extracted the words in the previous section, instead of that we can just extract out the noun phrases from the textblob. Noun Phrase extraction is particularly important when you want to analyze the â€œwhoâ€ in a sentence. Lets see an example below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "greyatom\n",
      "great platform\n",
      "data science\n"
     ]
    }
   ],
   "source": [
    "blob = TextBlob(\"GreyAtom is a great platform to learn data science.\")\n",
    "for np in blob.noun_phrases:\n",
    "     print (np)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part-of-speech Tagging\n",
    "Part-of-speech tagging or grammatical tagging is a method to mark words present in a text on the basis of its definition and context. In simple words, it tells whether a word is a noun, or an adjective, or a verb, etc. This is just a complete version of noun phrase extraction, where we want to find all the the parts of speech in a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GreyAtom NNP\n",
      "is VBZ\n",
      "a DT\n",
      "great JJ\n",
      "platform NN\n",
      "to TO\n",
      "learn VB\n",
      "data NNS\n",
      "science NN\n"
     ]
    }
   ],
   "source": [
    "for words, tag in blob.tags:\n",
    "    print (words, tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Words Inflection and Lemmatization\n",
    "Inflection is a process of word formation in which characters are added to the base form of a word to express grammatical meanings. Word inflection in TextBlob is very simple, i.e., the words we tokenized from a textblob can be easily changed into singular or plural."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "helps\n",
      "help\n"
     ]
    }
   ],
   "source": [
    "blob = TextBlob(\"GreyAtom is a great platform to learn data science. \\n It helps community through blogs, hackathons, discussions,etc.\")\n",
    "print (blob.sentences[1].words[1])\n",
    "print (blob.sentences[1].words[1].singularize())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Platforms'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textblob import Word\n",
    "w = Word('Platform')\n",
    "w.pluralize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'run'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## lemmatization\n",
    "w = Word('running')\n",
    "w.lemmatize(\"v\") ## v here represents verb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-grams\n",
    "A combination of multiple words together are called N-Grams. N grams (N > 1) are generally more informative as compared to words, and can be used as features for language modelling.  N-grams can be easily accessed in TextBlob using the ngrams function, which returns a tuple of n successive words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['GreyAtom', 'is']\n",
      "['is', 'a']\n",
      "['a', 'great']\n",
      "['great', 'platform']\n",
      "['platform', 'to']\n",
      "['to', 'learn']\n",
      "['learn', 'data']\n",
      "['data', 'science']\n",
      "['science', 'It']\n",
      "['It', 'helps']\n",
      "['helps', 'community']\n",
      "['community', 'through']\n",
      "['through', 'blogs']\n",
      "['blogs', 'hackathons']\n",
      "['hackathons', 'discussions']\n",
      "['discussions', 'etc']\n"
     ]
    }
   ],
   "source": [
    "for ngram in blob.ngrams(2):\n",
    "    print (ngram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis\n",
    "Sentiment analysis is basically the process of determining the attitude or the emotion of the writer, i.e., whether it is positive or negative or neutral.\n",
    "\n",
    "The sentiment function of textblob returns two properties, polarity, and subjectivity.\n",
    "\n",
    "Polarity is float which lies in the range of [-1,1] where 1 means positive statement and -1 means a negative statement. Subjective sentences generally refer to personal opinion, emotion or judgment whereas objective refers to factual information. Subjectivity is also a float which lies in the range of [0,1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GreyAtom is a great platform to learn data science. \n",
      " It helps community through blogs, hackathons, discussions,etc.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Sentiment(polarity=0.8, subjectivity=0.75)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print (blob)\n",
    "blob.sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VADER Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VADER (Valence Aware Dictionary and sEntiment Reasoner) is a lexicon and rule-based sentiment analysis tool that is specifically attuned to sentiments expressed in social media. VADER uses a combination of A sentiment lexicon is a list of lexical features (e.g., words) which are generally labelled according to their semantic orientation as either positive or negative.\n",
    "\n",
    "VADER has been found to be quite successful when dealing with social media texts, NY Times editorials, movie reviews, and product reviews. This is because VADER not only tells about the Positivity and Negativity score but also tells us about how positive or negative a sentiment is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advantages of using VADER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VADER has a lot of advantages over traditional methods of Sentiment Analysis, including:\n",
    "\n",
    "It works exceedingly well on social media type text, yet readily generalizes to multiple domains\n",
    "It doesnâ€™t require any training data but is constructed from a generalizable, valence-based, human-curated gold standard sentiment lexicon\n",
    "It is fast enough to be used online with streaming data, and\n",
    "It does not severely suffer from a speed-performance tradeoff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "analyser = SentimentIntensityAnalyzer()\n",
    "\n",
    "def sentiment_analyzer_scores(sentence):\n",
    "    score = analyser.polarity_scores(sentence)\n",
    "    print(\"{:-<40} {}\".format(sentence, str(score)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Machine Learning is nice---------------- {'neg': 0.0, 'neu': 0.517, 'pos': 0.483, 'compound': 0.4215}\n",
      "It is cool to learn Data Science-------- {'neg': 0.0, 'neu': 0.723, 'pos': 0.277, 'compound': 0.3182}\n",
      "The movie was terrible------------------ {'neg': 0.508, 'neu': 0.492, 'pos': 0.0, 'compound': -0.4767}\n"
     ]
    }
   ],
   "source": [
    "sentiment_analyzer_scores(\"Machine Learning is nice\")\n",
    "sentiment_analyzer_scores(\"It is cool to learn Data Science\")\n",
    "sentiment_analyzer_scores(\"The movie was terrible\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VADER analyses sentiments primarily based on certain key points:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Punctuation\n",
    "The use of an exclamation mark(!), increases the magnitude of the intensity without modifying the semantic orientation. For example, â€œThe food here is good!â€ is more intense than â€œThe food here is good.â€ and an increase in the number of (!), increases the magnitude accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The food here is good------------------- {'neg': 0.0, 'neu': 0.58, 'pos': 0.42, 'compound': 0.4404}\n",
      "The food here is good!------------------ {'neg': 0.0, 'neu': 0.556, 'pos': 0.444, 'compound': 0.4926}\n",
      "The food here is good!!----------------- {'neg': 0.0, 'neu': 0.534, 'pos': 0.466, 'compound': 0.5399}\n",
      "The food here is good!!!---------------- {'neg': 0.0, 'neu': 0.514, 'pos': 0.486, 'compound': 0.5826}\n"
     ]
    }
   ],
   "source": [
    "sentiment_analyzer_scores(\"The food here is good\")\n",
    "sentiment_analyzer_scores(\"The food here is good!\")\n",
    "sentiment_analyzer_scores(\"The food here is good!!\")\n",
    "sentiment_analyzer_scores(\"The food here is good!!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capitalization \n",
    "Using upper case letters to emphasize a sentiment-relevant word in the presence of other non-capitalized words, increases the magnitude of the sentiment intensity. For example, â€œThe food here is GREAT!â€ conveys more intensity than â€œThe food here is great!â€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The food here is great------------------ {'neg': 0.0, 'neu': 0.494, 'pos': 0.506, 'compound': 0.6249}\n",
      "The food here is GREAT------------------ {'neg': 0.0, 'neu': 0.453, 'pos': 0.547, 'compound': 0.7034}\n"
     ]
    }
   ],
   "source": [
    "sentiment_analyzer_scores(\"The food here is great\")\n",
    "sentiment_analyzer_scores(\"The food here is GREAT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Degree modifiers\n",
    "Also called intensifiers, they impact the sentiment intensity by either increasing or decreasing the intensity. For example, â€œThe service here is extremely goodâ€ is more intense than â€œThe service here is goodâ€, whereas â€œThe service here is marginally goodâ€ reduces the intensity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The service here is extremely good------ {'neg': 0.0, 'neu': 0.61, 'pos': 0.39, 'compound': 0.4927}\n",
      "The service here is marginally good----- {'neg': 0.0, 'neu': 0.657, 'pos': 0.343, 'compound': 0.3832}\n"
     ]
    }
   ],
   "source": [
    "sentiment_analyzer_scores(\"The service here is extremely good\")\n",
    "sentiment_analyzer_scores(\"The service here is marginally good\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conjunctions\n",
    "Use of conjunctions like â€œbutâ€ signals a shift in sentiment polarity, with the sentiment of the text following the conjunction being dominant. â€œThe food here is great, but the service is horribleâ€ has mixed sentiment, with the latter half dictating the overall rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The food here is great, but the service is horrible {'neg': 0.31, 'neu': 0.523, 'pos': 0.167, 'compound': -0.4939}\n"
     ]
    }
   ],
   "source": [
    "sentiment_analyzer_scores(\"The food here is great, but the service is horrible\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling Emojis, Slangs and Emoticons\n",
    "VADER performs very well with emojis, slangs and acronyms in sentences. Let us see each with an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am ðŸ˜„ today---------------------------- {'neg': 0.0, 'neu': 0.476, 'pos': 0.524, 'compound': 0.6705}\n",
      "None\n",
      "ðŸ˜Š--------------------------------------- {'neg': 0.0, 'neu': 0.333, 'pos': 0.667, 'compound': 0.7184}\n",
      "None\n",
      "ðŸ˜¥--------------------------------------- {'neg': 0.275, 'neu': 0.268, 'pos': 0.456, 'compound': 0.3291}\n",
      "None\n",
      "â˜¹ï¸-------------------------------------- {'neg': 0.706, 'neu': 0.294, 'pos': 0.0, 'compound': -0.34}\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(sentiment_analyzer_scores('I am ðŸ˜„ today'))\n",
    "print(sentiment_analyzer_scores('ðŸ˜Š'))\n",
    "print(sentiment_analyzer_scores('ðŸ˜¥'))\n",
    "print(sentiment_analyzer_scores('â˜¹ï¸'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Slangs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Today SUX!------------------------------ {'neg': 0.779, 'neu': 0.221, 'pos': 0.0, 'compound': -0.5461}\n",
      "None\n",
      "Today only kinda sux! But I'll get by, lol {'neg': 0.127, 'neu': 0.556, 'pos': 0.317, 'compound': 0.5249}\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(sentiment_analyzer_scores(\"Today SUX!\"))\n",
    "print(sentiment_analyzer_scores(\"Today only kinda sux! But I'll get by, lol\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emoticons\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Make sure you :) or :D today!----------- {'neg': 0.0, 'neu': 0.294, 'pos': 0.706, 'compound': 0.8633}\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(sentiment_analyzer_scores(\"Make sure you :) or :D today!\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<word form=\"great\" cornetto_synset_id=\"n_a-525317\" wordnet_id=\"a-01123879\" pos=\"JJ\" sense=\"very good\" polarity=\"1.0\" subjectivity=\"1.0\" intensity=\"1.0\" confidence=\"0.9\" />\n",
    "<word form=\"great\" wordnet_id=\"a-01278818\" pos=\"JJ\" sense=\"of major significance or importance\" polarity=\"1.0\" subjectivity=\"1.0\" intensity=\"1.0\" confidence=\"0.9\" />\n",
    "<word form=\"great\" wordnet_id=\"a-01386883\" pos=\"JJ\" sense=\"relatively large in size or number or extent\" polarity=\"0.4\" subjectivity=\"0.2\" intensity=\"1.0\" confidence=\"0.9\" />\n",
    "<word form=\"great\" wordnet_id=\"a-01677433\" pos=\"JJ\" sense=\"remarkable or out of the ordinary in degree or magnitude or effect\" polarity=\"0.8\" subjectivity=\"0.8\" intensity=\"1.0\" confidence=\"0.9\" />"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
